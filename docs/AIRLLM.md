# AirLLM Backend pro Shumilek

## 游 Co je AirLLM?

AirLLM umo쮄갓je spou코t캩t **70B modely na 4GB GPU** a **405B modely na 8GB VRAM** bez kvantizace, distilace nebo pruningu.

## 游닍 Instalace

```bash
# Z치kladn칤 instalace
pip install airllm flask

# Pro 4-bit/8-bit kompresi (3x rychlej코칤)
pip install bitsandbytes
```

## 游끢 Spu코t캩n칤 serveru

### Z치kladn칤 pou쬴t칤 (70B model)

```bash
cd scripts
python airllm_server.py --model "Qwen/Qwen2.5-72B-Instruct"
```

### S 4-bit kompres칤 (rychlej코칤, ~stejn치 kvalita)

```bash
python airllm_server.py \
  --model "Qwen/Qwen2.5-72B-Instruct" \
  --compression 4bit
```

### Llama 3.1 405B (pot콏eba 8GB VRAM)

```bash
python airllm_server.py \
  --model "meta-llama/Meta-Llama-3.1-405B" \
  --compression 4bit \
  --preload
```

### V코echny parametry

| Parametr | Popis | Default |
|----------|-------|---------|
| `--model, -m` | HuggingFace model ID | `Qwen/Qwen2.5-72B-Instruct` |
| `--compression, -c` | `4bit`, `8bit`, `none` | `none` |
| `--port, -p` | Port serveru | `11435` |
| `--host` | Host | `127.0.0.1` |
| `--max-length` | Max kontext | `2048` |
| `--preload` | Na캜칤st model ihned | `false` |
| `--delete-original` | Smazat HF cache | `false` |

## 丘뙖잺 Konfigurace Shumilek

V VS Code nastaven칤 (`Ctrl+,`):

```json
{
  "shumilek.baseUrl": "http://localhost:11435",
  "shumilek.model": "Qwen2.5-72B-Instruct"
}
```

Nebo v `settings.json`:

```json
{
  "shumilek.backendType": "airllm",
  "shumilek.airllm.serverUrl": "http://localhost:11435"
}
```

## 游늵 Podporovan칠 modely

| Model | VRAM pot콏eba | Komprese |
|-------|--------------|----------|
| Llama 2 70B | 4GB | voliteln치 |
| Llama 3 70B | 4GB | voliteln치 |
| Llama 3.1 405B | 8GB | doporu캜ena 4bit |
| Qwen 2.5 72B | 4GB | voliteln치 |
| Mixtral 8x22B | 4GB | voliteln치 |
| DeepSeek 67B | 4GB | voliteln치 |

## 游댢 API Endpointy

Server je **kompatibiln칤 s Ollama API**, tak쬰 Shumilek funguje bez 칰prav:

- `GET /api/tags` - Seznam model콢
- `POST /api/generate` - Generov치n칤 textu
- `POST /api/chat` - Chat API
- `POST /api/show` - Info o modelu
- `GET /health` - Health check

## 丘멆잺 Pozn치mky

1. **Prvn칤 spu코t캩n칤** trv치 d칠le - model se transformuje na vrstvy
2. **Disk space** - pot콏eba ~2x velikost modelu pro transformaci
3. **Inference je pomalej코칤** ne Ollama s kvantizovan칳mi modely
4. **Vhodn칠 pro** kvalitn칤 odpov캩di, ne rychl칠 iterace

## 游냍 Troubleshooting

### MetadataIncompleteBuffer error
```
Do코el disk. Uvolni m칤sto a sma HuggingFace cache:
rm -rf ~/.cache/huggingface
```

### CUDA out of memory
```
Zkus 4-bit kompresi:
python airllm_server.py --compression 4bit
```

### Model nenalezen
```
Pro gated modely (Llama) pot콏ebuje코 HF token:
export HF_TOKEN=your_token_here
```

## Cache and precision (optional)

Example settings:

```json
{
  "shumilek.airllm.cacheDir": "C:\\AI\\hf",
  "shumilek.airllm.dtype": "bf16"
}
```
